{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x0000022E7F96BCE0>\n",
      "Train the Naive Bayes classifier\n",
      "Trained NaiveBayes_Classifier\n",
      "Training SVC_classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\scian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained SVC_classifier\n",
      "Training Logisitic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\scian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\scian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Logisitic Regression\n",
      "Training MNB_classifier\n",
      "Trained MNB_classifier\n",
      "Training SGDClassifier_classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\scian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained SGDClassifier_classifier\n",
      "Training LinearSVC_classifier\n",
      "Trained LinearSVC_classifier\n",
      "Training BernoulliNB_classifier\n",
      "Trained BernoulliNB_classifier\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import pprint\n",
    "import nltk.classify\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "#start replaceTwoOrMore\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "#start process_tweet\n",
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "    \n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)    \n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#end \n",
    "\n",
    "#start getStopWordList\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "#end\n",
    "\n",
    "#start getfeatureVector\n",
    "def getFeatureVector(tweet, stopWords):\n",
    "    featureVector = []\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences \n",
    "        w = replaceTwoOrMore(w) \n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if it consists of only words\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*[a-zA-Z]+[a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stopWord\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector    \n",
    "#end\n",
    "\n",
    "#start extract_features\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "#end\n",
    "\n",
    "dircetory = \"C:\\\\Users\\\\scian\\\\Desktop\\\\Research Assistant\\\\Twitter-Sentiment-Analysis-on-Brexit\\\\\"\n",
    "\n",
    "#Read the tweets one by one and process it\n",
    "inpTweets = csv.reader(open(dircetory + 'training_final.csv', 'r', encoding = \"cp850\"))\n",
    "# inpTweets = pd.read_csv(\"data/full_training_dataset.csv\", encoding=\"\")\n",
    "print(inpTweets)\n",
    "stopWords = getStopWordList(dircetory + 'stopwords.txt')\n",
    "count = 0\n",
    "featureList = []\n",
    "tweets = []\n",
    "\n",
    "for row in inpTweets:\n",
    "    # print(row)\n",
    "    sentiment = row[0]\n",
    "    tweet = row[1]\n",
    "    processedTweet = processTweet(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet, stopWords)\n",
    "    featureList.extend(featureVector)\n",
    "    tweets.append((featureVector, sentiment))\n",
    "#end loop\n",
    "\n",
    "# Remove featureList duplicates\n",
    "featureList = list(set(featureList))\n",
    "# print(\"featureList\", featureList)\n",
    "\n",
    "# Generate the training set\n",
    "training_set = nltk.classify.util.apply_features(extract_features, tweets)\n",
    "# print(\"training set\", training_set)\n",
    "\n",
    "\n",
    "print(\"Train the Naive Bayes classifier\")\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Trained NaiveBayes_Classifier\")\n",
    "\n",
    "filename = 'NaiveBayes_Classifier.sav'\n",
    "pickle.dump(NBClassifier, open(dircetory + filename, 'wb'))\n",
    "\n",
    "\n",
    "print(\"Training SVC_classifier\")\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"Trained SVC_classifier\")\n",
    "\n",
    "filename1 = 'SVC_classifier.sav'\n",
    "pickle.dump(SVC_classifier, open(dircetory + filename1, 'wb'))\n",
    "\n",
    "print(\"Training Logisitic Regression\")\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"Trained Logisitic Regression\")\n",
    "\n",
    "filename3 = 'LogisticRegression_classifier.sav'\n",
    "pickle.dump(LogisticRegression_classifier, open(dircetory + filename3, 'wb'))\n",
    "\n",
    "\n",
    "print(\"Training MNB_classifier\")\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"Trained MNB_classifier\")\n",
    "\n",
    "filename4 = 'MNB_classifier.sav'\n",
    "pickle.dump(MNB_classifier, open(dircetory + filename4, 'wb'))\n",
    "\n",
    "\n",
    "print(\"Training SGDClassifier_classifier\")\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"Trained SGDClassifier_classifier\")\n",
    "\n",
    "filename5 = 'SGDClassifier_classifier.sav'\n",
    "pickle.dump(SGDClassifier_classifier, open(dircetory + filename5, 'wb'))\n",
    "\n",
    "\n",
    "print(\"Training LinearSVC_classifier\")\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"Trained LinearSVC_classifier\")\n",
    "\n",
    "filename6 = 'LinearSVC_classifier.sav'\n",
    "pickle.dump(LinearSVC_classifier, open(dircetory + filename6, 'wb'))\n",
    "\n",
    "\n",
    "print(\"Training BernoulliNB_classifier\")\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"Trained BernoulliNB_classifier\")\n",
    "\n",
    "filename7 = 'BernoulliNB_classifier.sav'\n",
    "pickle.dump(BernoulliNB_classifier, open(dircetory + filename7, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TFidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import Word\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    \n",
    "    string = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',string)\n",
    "    string = re.sub('@[^\\s]+','',string)    \n",
    "    string = re.sub(r'#([^\\s]+)', r'\\1', string)\n",
    "    string = re.sub(r\"\\'s\", \"\", string)\n",
    "    string = re.sub(r\"\\'ve\", \"\", string)\n",
    "    string = re.sub(r\"n\\'t\", \"\", string)\n",
    "    string = re.sub(r\"\\'re\", \"\", string)\n",
    "    string = re.sub(r\"\\'d\", \"\", string)\n",
    "    string = re.sub(r\"\\'ll\", \"\", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \"\", string)\n",
    "    string = re.sub(r\"'\", \"\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"[0-9]\\w+|[0-9]\",\"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "data = pd.read_csv(\"training_final.csv\")\n",
    "x = data['tweets'].tolist()\n",
    "y = data['sentiment'].tolist()\n",
    "\n",
    "for index,value in enumerate(x):\n",
    "    x[index] = ' '.join([Word(word).lemmatize() for word in clean_str(value).split()])\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english',min_df=2)\n",
    "vect.fit(x)\n",
    "X = vect.transform(x)\n",
    "Y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vect,open(\"us-polti.vect\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6003, 5852)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=150, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, max_depth=150,n_jobs=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open(\"Tfidf-RandomForestClassifier.sav\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
